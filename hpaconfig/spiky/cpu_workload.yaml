# defaults
behavior:
  scaleDown:
    stabilizationWindowSeconds: 300
    policies:
      - type: Percent
        value: 100
        periodSeconds: 15
  scaleUp:
    stabilizationWindowSeconds: 0
    policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
    selectPolicy: Max

# way to many pods (12 permanently), average of 50% CPU load
cpu_1:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 12
    targetCPUUtilizationPercentage: 80
    scaleUpWindow: 0
    scaleDownWindow: 300

# max of 7 pods, but lag did increase endlessly at 30k messages and HPA did not scale up
# CPU load was good at an average of 80-85%
# Scale down in the end did not take place fast enough
cpu_2:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 12
    targetCPUUtilizationPercentage: 85
    scaleUpWindow: 60
    scaleDownWindow: 120

# average util was at roundabout 70%
#
# scale down seems too fast -> with a lot of instances, when re-balancing takes place, the consumerapps have a very low
# utilization for a short period of time -> overall utilization is below threshold, scale down starts, lag increases
# very fast -> wenn re-balancing is finished, nodes have way too much load, so new instances are created and re-balancing
# happens again
#
# but also, scale down is too slow after a strong workload spike - it does not scale down at all and average utilization
# goes below 50%
#
cpu_3:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 24
    targetCPUUtilizationPercentage: 80
    scaleUp:
      scaleUpWindow: 30
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
    scaleDown:
      scaleDownWindow: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15

# on startup, there is still some major lag building up
#
# scaledown seems a bit too volatile, but hard to estimate - lag is at least under control, replicas are
# at an all-time minimum
#
# average level of replicas was at ann all-time minimum, scale down happened very fast, lag was under control for most
# time
#
# average CPU utilization was higher, at roundabout 70%, as well as average message latency
#
cpu_4:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 24
    targetCPUUtilizationPercentage: 80
    scaleUp:
      scaleUpWindow: 30
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
        - type: Pods
          value: 4
          periodSeconds: 15
    scaleDown:
      scaleDownWindow: 30
      policies:
        - type: Percent
          value: 100
          periodSeconds: 120
        - type: Pods
          value: 2
          periodSeconds: 15

# no lag building up
# amount of consumers is way higher, no scale-downs in increasing message phase
# message latency lower, not so many spikes
# cpu util lower, only because of max limit of 24 replicas is hit at 40k messages, the load is roundabout 70%
# 5 Minutes after message rate lowered, still now scaledown observable
# -> because of "spiking" workload which creates a very-short term high utilization and the behaviour of the HPA to take
#    the highest values, scale down does not happen properly
# -> even when the average workload is below 40%, it does not scale down!
# -> back at 20-10k messages, it finally scales down
#
cpu_5:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 24
    targetCPUUtilizationPercentage: 80
    scaleUp:
      scaleUpWindow: 15
      policies:
        - type: Percent
          value: 100
          periodSeconds: 60
        - type: Pods
          value: 4
          periodSeconds: 15
    scaleDown:
      scaleDownWindow: 90
      policies:
        - type: Percent
          value: 100
          periodSeconds: 120
        - type: Pods
          value: 2
          periodSeconds: 30

cpu_6:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 24
    targetCPUUtilizationPercentage: 90
    scaleUp:
      scaleUpWindow: 30
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 4
          periodSeconds: 30
    scaleDown:
      scaleDownWindow: 45
      policies:
        - type: Percent
          value: 100
          periodSeconds: 120
        - type: Pods
          value: 2
          periodSeconds: 45
