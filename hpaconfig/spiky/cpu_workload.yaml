# way to many pods (12 permanently), average of 50% CPU load
cpu_1:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 12
    targetCPUUtilizationPercentage: 80
    scaleUpWindow: 0
    scaleDownWindow: 300

# max of 7 pods, but lag did increase endlessly at 30k messages and HPA did not scale up
# CPU load was good at an average of 80-85%
# Scale down in the end did not take place fast enough
cpu_2:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 12
    targetCPUUtilizationPercentage: 85
    scaleUpWindow: 60
    scaleDownWindow: 120

# average util was at roundabout 70%
#
# scale down seems too fast -> with a lot of instances, when re-balancing takes place, the consumerapps have a very low
# utilization for a short period of time -> overall utilization is below threshold, scale down starts, lag increases
# very fast -> wenn re-balancing is finished, nodes have way too much load, so new instances are created and re-balancing
# happens again
#
# but also, scale down is too slow after a strong workload spike - it does not scale down at all and average utilization
# goes below 50%
#
cpu_3:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 24
    targetCPUUtilizationPercentage: 80
    scaleUp:
      scaleUpWindow: 30
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
    scaleDown:
      scaleDownWindow: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15

# on startup, there is still some major lag building up
#
# scaledown seems a bit too volatile, but hard to estimate - lag is at least under control, replicas are
# at an all-time minimum
#
# average level of replicas was at ann all-time minimum, scale down happened very fast, lag was under control for most
# time
#
# average CPU utilization was higher, at roundabout 70%, as well as average message latency
#
cpu_4:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 24
    targetCPUUtilizationPercentage: 80
    scaleUp:
      scaleUpWindow: 30
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
        - type: Pods
          value: 4
          periodSeconds: 15
    scaleDown:
      scaleDownWindow: 30
      policies:
        - type: Percent
          value: 100
          periodSeconds: 120
        - type: Pods
          value: 2
          periodSeconds: 15

# no lag building up
# amount of consumers is way higher, no scale-downs in increasing message phase
# message latency lower, not so many spikes
# cpu util lower, only because of max limit of 24 replicas is hit at 40k messages, the load is roundabout 70%
# 5 Minutes after message rate lowered, still now scaledown observable
# -> because of "spiking" workload which creates a very-short term high utilization and the behaviour of the HPA to take
#    the highest values, scale down does not happen properly
# -> even when the average workload is below 40%, it does not scale down!
# -> back at 20-10k messages, it finally scales down
#
cpu_5:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 24
    targetCPUUtilizationPercentage: 80
    scaleUp:
      scaleUpWindow: 15
      policies:
        - type: Percent
          value: 100
          periodSeconds: 60
        - type: Pods
          value: 4
          periodSeconds: 15
    scaleDown:
      scaleDownWindow: 90
      policies:
        - type: Percent
          value: 100
          periodSeconds: 120
        - type: Pods
          value: 2
          periodSeconds: 30


# !!! LOOKS GOOD FIRST, TURNS OUT AS BAD SOLUTION
# seems like the best configuration so far
# little lag build up
# lowest amount of instances, way lower than in cpu_5 run
# -> maximum of 15! instances, compared to 24 in other runs
# average utilization was between 80-85%
# latency was totally fine, even better than in other runs
# scale down was not too aggressive, rather kinda accurate
#
# 90% seems to high, at 40k messages it does not scale fast "enough", does not scale up at all and lag increases endlessly
# -> scale up window may too big in combination with 90% target load
#
cpu_6:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 24
    targetCPUUtilizationPercentage: 90
    scaleUp:
      scaleUpWindow: 30
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 4
          periodSeconds: 30
    scaleDown:
      scaleDownWindow: 45
      policies:
        - type: Percent
          value: 100
          periodSeconds: 120
        - type: Pods
          value: 2
          periodSeconds: 45

# scale up seems to be too aggressive
# scaling down is better than in cpu_5 and not as aggressive as in cpu_4.
# no lag was building up at all
cpu_7:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 24
    targetCPUUtilizationPercentage: 85
    scaleUp:
      scaleUpWindow: 15
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 4
          periodSeconds: 30
    scaleDown:
      scaleDownWindow: 45
      policies:
        - type: Percent
          value: 100
          periodSeconds: 120
        - type: Pods
          value: 2
          periodSeconds: 45

# The problem is: if some pods are below the limit, some are constantly at 100, the scale up does not happen properly
# one problem could is that messages are not distributed equally across partitions, which is a close-to-reality scenario
# lag can build up and even though no scale up happens, because some apps get tons of messages, others don't
cpu_8:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 24
    targetCPUUtilizationPercentage: 85
    scaleUp:
      scaleUpWindow: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 4
          periodSeconds: 30
    scaleDown:
      scaleDownWindow: 45
      policies:
        - type: Percent
          value: 100
          periodSeconds: 120
        - type: Pods
          value: 2
          periodSeconds: 45

# after this point, the batch size and linger were set to a minimum to ensure better partition distribution
# but, it seems like it is still a problem, at high message rates, some consumers are allways at 100%, a lot of them
# are not
cpu_9:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 24
    targetCPUUtilizationPercentage: 70
    scaleUp:
      scaleUpWindow: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 4
          periodSeconds: 30
    scaleDown:
      scaleDownWindow: 45
      policies:
        - type: Percent
          value: 100
          periodSeconds: 120
        - type: Pods
          value: 2
          periodSeconds: 45

cpu_10:
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 24
    targetCPUUtilizationPercentage: 60
    scaleUp:
      scaleUpWindow: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 4
          periodSeconds: 30
    scaleDown:
      scaleDownWindow: 45
      policies:
        - type: Percent
          value: 100
          periodSeconds: 120
        - type: Pods
          value: 2
          periodSeconds: 45
